{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.7.0\n",
      "  Using cached torch-1.7.0-cp36-cp36m-win_amd64.whl (184.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sakuma\\anaconda3\\lib\\site-packages)\n",
      "ERROR: torch has an invalid wheel, .dist-info directory not found\n",
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sakuma\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement torchvision==0.8.1 (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.3.0, 0.4.1, 0.5.0, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.11.1, 0.11.2)\n",
      "ERROR: No matching distribution found for torchvision==0.8.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\_internally_replaced_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.hub'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-91255bdd0d10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# バージョンの確認\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\extension.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_internally_replaced_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_get_extension_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\_internally_replaced_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_url\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mthroughput_benchmark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mThroughputBenchmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_crash_handler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menable_minidumps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable_minidumps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menable_minidumps_on_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\throughput_benchmark.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mformat_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_us\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'''Defines how to format time'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "# バージョン指定時にコメントアウト\n",
    "!pip install torch==1.7.0\n",
    "!pip install torchvision==0.8.1\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "# バージョンの確認\n",
    "print(torch.__version__) \n",
    "print(torchvision.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-46c0cffdbfb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# パッケージのインポート\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torchsummary\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定\n",
    "workers = 0\n",
    "batch_size=50\n",
    "nz = 100\n",
    "nch_g = 128\n",
    "nch_d = 128\n",
    "n_epoch = 20\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "outf = './result13_DCGAN'\n",
    "display_interval = 600\n",
    "\n",
    "# 保存先ディレクトリを作成\n",
    "try:\n",
    "    os.makedirs(outf, exist_ok=True)\n",
    "except OSError as error: \n",
    "    print(error)\n",
    "    pass\n",
    "\n",
    "# 乱数のシード（種）を固定\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasetに心音画像を保存する\n",
    "\n",
    "dataset = dset.ImageFolder(root='C:/Users/sakuma/research/DCGAN_test/dc_gan/trim_normal_200',\n",
    "                            transform=transforms.Compose([\n",
    "                           transforms.RandomResizedCrop(28, scale=(1.0, 1.0), ratio=(1., 1.)),\n",
    "                           transforms.RandomHorizontalFlip(),#左右逆転した画像を生成\n",
    "                           transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5,), (0.5,)),\n",
    "                       ])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageFolder(Dataset):\n",
    "    IMG_EXTENSIONS = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        # 画像ファイルのパス一覧を取得する。\n",
    "        self.img_paths = self._get_img_paths(img_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_paths[index]\n",
    "\n",
    "        # 画像を読み込む。\n",
    "        img = Image.open(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # 前処理がある場合は行う。\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def _get_img_paths(self, img_dir):\n",
    "        \"\"\"指定したディレクトリ内の画像ファイルのパス一覧を取得する。\n",
    "        \"\"\"\n",
    "        img_dir = Path(img_dir)\n",
    "        img_paths = [\n",
    "            p for p in img_dir.iterdir() if p.suffix in ImageFolder.IMG_EXTENSIONS\n",
    "        ]\n",
    "\n",
    "        return img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"ディレクトリ内の画像ファイルの数を返す。\n",
    "        \"\"\"\n",
    "        return len(self.img_paths)\n",
    "\n",
    "\n",
    "# Transform を作成する。\n",
    "transform=transforms.Compose([\n",
    "                          transforms.RandomResizedCrop(28, scale=(1.0, 1.0), ratio=(1., 1.)),\n",
    "                          transforms.RandomHorizontalFlip(),#左右逆転した画像を生成\n",
    "                          transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5,), (0.5,)),\n",
    "                       ])\n",
    "# Dataset を作成する。\n",
    "dataset = ImageFolder(\"C:/Users/sakuma/research/DCGAN_test/dc_gan/trim_normal_200\", transform)\n",
    "# DataLoader を作成する。\n",
    "#dataloader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "#for batch in dataloader:\n",
    "#print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像配列の確認\n",
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.unsqueeze(dim=0)\n",
    "print(x.shape)\n",
    "#torch.Size([1, 3, 256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の域値(-1～1)の確認\n",
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データをセットしたデータローダーを作成する\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=int(workers))\n",
    "\n",
    "# 学習に使用するデバイスを得る。可能ならGPUを使用する\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# 訓練データをランダムに取得\n",
    "dataiter = iter(dataloader)\n",
    "images = dataiter.next()\n",
    "\n",
    "# 画像の表示\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# ラベルの表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    生成器Gのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nz=100, nch_g=128, nch=3):\n",
    "        \"\"\"\n",
    "        :param nz: 入力ベクトルzの次元\n",
    "        :param nch_g: 最終層の入力チャネル数\n",
    "        :param nch: 出力画像のチャネル数\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # ニューラルネットワークの構造を定義する\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nz, nch_g * 4, 3, 1, 0),     # 転置畳み込み\n",
    "                nn.BatchNorm2d(nch_g * 4),                      # バッチノーマライゼーション\n",
    "                nn.ReLU()                                       # ReLU\n",
    "            ),  # (B, nz, 1, 1) -> (B, nch_g*4, 3, 3)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 4, nch_g * 2, 3, 2, 0),\n",
    "                nn.BatchNorm2d(nch_g * 2),\n",
    "                nn.ReLU()\n",
    "            ),  # (B, nch_g*4, 3, 3) -> (B, nch_g*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g * 2, nch_g, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nch_g),\n",
    "                nn.ReLU()\n",
    "            ),  # (B, nch_g*2, 7, 7) -> (B, nch_g, 14, 14)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.ConvTranspose2d(nch_g, nch, 4, 2, 1),\n",
    "                nn.Tanh()\n",
    "            )   # (B, nch_g, 14, 14) -> (B, nch, 28, 28)\n",
    "        })\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        :param z: 入力ベクトル\n",
    "        :return: 生成画像\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersの各層で演算を行う\n",
    "            z = layer(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    識別器Dのクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, nch=3, nch_d=128):\n",
    "        \"\"\"\n",
    "        :param nch: 入力画像のチャネル数\n",
    "        :param nch_d: 先頭層の出力チャネル数\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # ニューラルネットワークの構造を定義する\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'layer0': nn.Sequential(\n",
    "                nn.Conv2d(nch, nch_d, 4, 2, 1),     # 畳み込み\n",
    "                nn.LeakyReLU(negative_slope=0.2)    # leaky ReLU関数\n",
    "            ),  # (B, nch, 28, 28) -> (B, nch_d, 14, 14)\n",
    "            'layer1': nn.Sequential(\n",
    "                nn.Conv2d(nch_d, nch_d * 2, 4, 2, 1),\n",
    "                nn.BatchNorm2d(nch_d * 2),\n",
    "                nn.LeakyReLU(negative_slope=0.2)\n",
    "            ),  # (B, nch_d, 14, 14) -> (B, nch_d*2, 7, 7)\n",
    "            'layer2': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 2, nch_d * 4, 3, 2, 0),\n",
    "                nn.BatchNorm2d(nch_d * 4),\n",
    "                nn.LeakyReLU(negative_slope=0.2)\n",
    "            ),  # (B, nch_d*2, 7, 7) -> (B, nch_d*4, 3, 3)\n",
    "            'layer3': nn.Sequential(\n",
    "                nn.Conv2d(nch_d * 4, 1, 3, 1, 0),\n",
    "                nn.Sigmoid()    # Sigmoid関数\n",
    "            )    \n",
    "            # (B, nch_d*4, 3, 3) -> (B, 1, 1, 1)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        順方向の演算\n",
    "        :param x: 本物画像あるいは生成画像\n",
    "        :return: 識別信号\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():  # self.layersの各層で演算を行う\n",
    "            x = layer(x)\n",
    "        return x.squeeze()     # Tensorの形状を(B)に変更して戻り値とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    ニューラルネットワークの重みを初期化する。作成したインスタンスに対しapplyメソッドで適用する\n",
    "    :param m: ニューラルネットワークを構成する層\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:            # 畳み込み層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:        # 全結合層の場合\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:     # バッチノーマライゼーションの場合\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器G　ランダムベクトルから生成画像を作成する\n",
    "netG = Generator(nz=nz, nch_g=nch_g).to(device)\n",
    "netG.apply(weights_init)    # weights_init関数で初期化\n",
    "#print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別器D　画像が本物画像か生成画像かを識別する\n",
    "netD = Discriminator(nch_d=nch_d).to(device)\n",
    "netD.apply(weights_init)\n",
    "#print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 識別器DのTensor形状\n",
    "torchsummary.summary(netD, (3, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()    # バイナリークロスエントロピー（Sigmoid関数無し）\n",
    "\n",
    "# 生成器のエポックごとの画像生成に使用する確認用の固定ノイズ\n",
    "fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)  \n",
    "\n",
    "# オプティマイザ−のセットアップ\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 識別器D用\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 生成器G用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "D_x_out = []\n",
    "D_G_z1_out = []\n",
    "\n",
    "# 学習のループ\n",
    "for epoch in range(n_epoch):\n",
    "    for itr, data in enumerate(dataloader):\n",
    "        real_image = data[0].to(device)     # 本物画像\n",
    "        sample_size = real_image.size(0)    # 画像枚数\n",
    "        \n",
    "        # 標準正規分布からノイズを生成\n",
    "        noise = torch.randn(sample_size, nz, 1, 1, device=device)\n",
    "        # 本物画像に対する識別信号の目標値「1」\n",
    "        real_target = torch.full((sample_size,), 1., device=device)\n",
    "        # 生成画像に対する識別信号の目標値「0」\n",
    "        fake_target = torch.full((sample_size,), 0., device=device) \n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # 識別器Dの更新\n",
    "        ###########################\n",
    "        netD.zero_grad()    # 勾配の初期化\n",
    "\n",
    "        output = netD(real_image)   # 識別器Dで本物画像に対する識別信号を出力\n",
    "        print(output)\n",
    "        print(real_target)\n",
    "        errD_real = criterion(output, real_target)  # 本物画像に対する識別信号の損失値\n",
    "        D_x = output.mean().item()  # 本物画像の識別信号の平均\n",
    "\n",
    "        fake_image = netG(noise)    # 生成器Gでノイズから生成画像を生成\n",
    "        \n",
    "        output = netD(fake_image.detach())  # 識別器Dで本物画像に対する識別信号を出力\n",
    "        errD_fake = criterion(output, fake_target)  # 生成画像に対する識別信号の損失値\n",
    "        D_G_z1 = output.mean().item()  # 生成画像の識別信号の平均\n",
    "\n",
    "        errD = errD_real + errD_fake    # 識別器Dの全体の損失\n",
    "        errD.backward()    # 誤差逆伝播\n",
    "        optimizerD.step()   # Dのパラメーターを更新\n",
    "\n",
    "        ############################\n",
    "        # 生成器Gの更新\n",
    "        ###########################\n",
    "        netG.zero_grad()    # 勾配の初期化\n",
    "        \n",
    "        output = netD(fake_image)   # 更新した識別器Dで改めて生成画像に対する識別信号を出力\n",
    "        errG = criterion(output, real_target)   # 生成器Gの損失値。Dに生成画像を本物画像と誤認させたいため目標値は「1」\n",
    "        errG.backward()     # 誤差逆伝播\n",
    "        D_G_z2 = output.mean().item()  # 更新した識別器Dによる生成画像の識別信号の平均\n",
    "\n",
    "        optimizerG.step()   # Gのパラメータを更新\n",
    "\n",
    "        if itr % display_interval == 0: \n",
    "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
    "                  .format(epoch + 1, n_epoch,\n",
    "                          itr + 1, len(dataloader),\n",
    "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        if epoch == 0 and itr == 0:     # 初回に本物画像を保存する\n",
    "            vutils.save_image(real_image, '{}/real_samples.png'.format(outf),\n",
    "                              normalize=True, nrow=10)\n",
    "\n",
    "        # ログ出力用データの保存\n",
    "        D_losses.append(errD.item())\n",
    "        G_losses.append(errG.item())\n",
    "        D_x_out.append(D_x)\n",
    "        D_G_z1_out.append(D_G_z1)\n",
    "\n",
    "    ############################\n",
    "    # 確認用画像の生成\n",
    "    ############################\n",
    "    fake_image = netG(fixed_noise)  # 1エポック終了ごとに確認用の生成画像を生成する\n",
    "    vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n",
    "                      normalize=True, nrow=10)\n",
    "\n",
    "    ############################\n",
    "    # モデルの保存\n",
    "    ############################\n",
    "    if (epoch + 1) % 10 == 0:   # 10エポックごとにモデルを保存する\n",
    "        torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "        torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
